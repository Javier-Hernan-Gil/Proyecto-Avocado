# -*- coding: utf-8 -*-
"""Copia de Proyecto - Mercado de Aguacate en Estados Unidos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wCf37UxrxbBZgMYVwoMkG4CdeYVyd0kP

![](https://drive.google.com/uc?export=view&id=1ugpRfZ7DXMLZPpATxvtZfCUIMv871b0s)

HAB es la única organización de aguacate que equipa a toda la industria global para el éxito al recolectar, enfocar y distribuir inversiones para mantener y expandir la demanda de aguacates en los Estados Unidos.

HAB proporciona a la industria datos consolidados de suministro y mercado, realiza investigaciones sobre nutrición, educa a los profesionales de la salud y reúne a personas de todos los rincones de la industria para trabajar colectivamente hacia un crecimiento que beneficie a todos. La organización también recauda y reasigna fondos a California y asociaciones de importadores para beneficiar a países de origen específicos en la promoción de sus marcas de aguacate a clientes y consumidores en todo Estados Unidos.

### **Misión**
 
<p align="justify"> 
HAB existe para apoyar a las partes interesadas de la industria mundial del aguacate en nuestros esfuerzos colectivos hacia la expansión del mercado en los EE. UU. Proporcionan a los productores datos de volumen, proyecciones, conocimientos de mercado y otras herramientas comerciales para ayudarlos a tomar buenas decisiones comerciales.</p>  



### **Visión**
<p align="justify"> 
HAB es el catalizador para que los aguacates sean la fruta más consumida en los EE. UU. Y las partes interesadas de la industria tengan éxito. Brindando a los productores, comercializadores, profesionales de la salud y consumidores datos, investigaciones e información fiables e imparciales sobre el aguacate para impulsar el crecimiento de la categoría. </p> 

### **Objetivos Estratégicos**


Seis prioridades estratégicas respaldan la visión a largo plazo de la Junta de ser el catalizador para que los aguacates frescos sean la fruta consumida número uno en los EE. UU. Y las partes interesadas de la industria tengan éxito:

* Construir demanda
* Nutrición
* Datos de oferta y demanda
* Calidad
* Compromiso de la industria
* Sustentabilidad 

**Fuente:** https://hassavocadoboard.com/

# **Proyecto -  "Avocado Markets in the United States"**

<p align="justify">  El proyecto de análisis de datos sobre  Aguacate Hass en Estados Unidos se basa en los datos obtenidos por HAB, la cual  apoya a los proveedores de aguacate con programas de datos estratégicos, de calidad, de oferta y de demanda para que puedan tomar mejores decisiones al administrar su negocio. proporcionan información que es vital para el éxito de la industria, ya sean datos de inventario a corto plazo o datos de consumidores a largo plazo que ayudan a todos en la cadena de suministro a vigilar al consumidor y sus impulsores de compra.

<p align="justify"> El Informe de datos de categoría proporciona datos procesables a la industria sobre cuándo, dónde y cuánto aguacate vende el canal minorista, tanto en libras como en dólares. Su objetivo es rastrear y monitorear el desempeño del aguacate Hass en el comercio minorista en los Estados Unidos, dividido en 8 regiones separadas y 45 mercados individuales. </p>

## **Metas del Proyecto**
<p align="justify">
El desarrollo del proyecto está enmarcado en tres fases, en cada una de ellas se realizan una serie de actividades que permiten alcanzar los objetivos del proyecto. La información central para realizar dicho proyecto, se basa en un Dataset que contiene el precio promedio por unidad de aguacáte clase Hass en algunas regiones y/o cidudades de Estados Unidades entre el período 2015-01-04 y 2018-03-25, entre otras características.</p>

## **Objetivos del Proyecto**

* Construir una serie de tiempo que permita identifcar el comportamiento del precio promedio del aguacate entre el 4  de enero de 2015 al 25 de Marzo del 2018.

* Entrenar un modelo autorregresivo que permita predecir la variable 'precio promedio por unidad' determinando el valor que tomará la variable en el futuro, cuya evaluación se sustentará en una validación cruzada mediante sliding window y forward chaining validation

##**Fases del Proyecto**
Para el desarrollo se plantean algunas actividades generales en las siguientes fases:
### **Fase 1. Selección y análisis exploratorio de los datos** 

1. Presentar la información general del dataset.
2. Selecionar las variables relevantes para el análisis.
3. Realizar un análisis exploratorio  de las variables del dataset, mediante gráficos y algunas estadísticas descriptivas.

### **Fase 2. Preparación y Procesamiento de los datos** 
1. Efectuar el proceso de limpieza y filtarado de dataset, que permita identificar si hay valores faltantes, incorrectos o con ruido.
2. Renombrar algunas variables y registros que permitan uniformidad en los datos.

### **Fase 3. Análisis y modelamiento de los datos** 
1. Construir la serie de tiempo de los precios promedios por unidad del aguacate tipo Hass en algunas regiones de los Estados Unidos.
2. Utilizar el Método Multilayer Perceptron Regressor (MLPR) junto GridSearchCV para encontrar los mejores parámetros.
3. Utilizar el validador cruzado de series temporales TimeSeriesSplit de scikit-learn realizando una validación tipo forward chaining.

### **Fase 4. Evaluación y resultados** 
1. Realizar predicion del precio promedio basado en particiones de los datos conocidos.
2. Evaluar el modelo mediante algunas métricas de desempeño.
3. Presentar algunas consideraciones iniciales según el desempeño del modelo.

##**Instalación de Paquetes y Librerias**

Para iniciar el análisis, se instalan los paquetes y librerias necesarias.
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install -U plotly # Versión más reciente de plotly
import numpy as np
import pandas as pd
import datetime
import seaborn as sns 
import plotly
import plotly.graph_objs as go 
import plotly.express as px
import matplotlib.pyplot as plt
# %matplotlib inline

# Selección del modelo
from sklearn.model_selection import train_test_split   #Subconjuntos de entrenamiento y pruebas.
from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold #Validación cruzada de K-pliegues.
from sklearn.model_selection import GridSearchCV  #Búsqueda en cuadrícula de hiperparámetros.

# Selección de los datos en series de tiempo
from sklearn.model_selection import TimeSeriesSplit

# Regresores
from sklearn.neural_network import MLPRegressor    #Regresor con red neuronal de tipo percetron multicapa.

# Métricas de rendimiento
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error #Error absoluto, cuadrado, y cuadrado logarítmico.

"""El análisis realizado tuvo como base un conjunto de datos registrado en Kaggle, obtenido de la página oficial de Hass Avocado Board  en http://www.hassavocadoboard.com/retail/volume-and-price-data por lo cual es un proyecto netamente academico sin fines comerciales.
Este material fue realizado con las siguientes versiones:

- Python 3.6.9

- Pandas 1.0.5

- NumPy 1.18.5

- Seaborn 0.10.1

- Plotly 0.10.1

- Scipy 0.10.1


"""

!python --version
print('Pandas', pd.__version__)
print('numpy', np.__version__)
print('seaborn', sns.__version__)
print('plotly', sns.__version__)
print('scipy', sns.__version__)

"""# **Fase 1. Entendimiento de los datos**

### **1. Recoleción inicial de los datos**

<p align="justify"> En esta fase se realizará una exploración inicial del dataset "avocado.csv" obtenido de Kaggle, el cual puede ser verficado de la página oficial de Hass Avocado Board  en http://www.hassavocadoboard.com/retail/volume-and-price-data. La siguiente tabla representa los datos de escaneo minorista semanales entre los años 2015 a 2018 para el volumen minorista nacional (unidades) y el precio. Los datos de escaneo minorista provienen directamente de las cajas registradoras de los minoristas en función de las ventas minoristas reales de aguacates Hass.</p>
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)
datos= pd.read_excel("/content/drive/MyDrive/Curso_ML_Avanzado/Metodologias/Avocado.xlsx")

dff=datos.copy()

"""####**Información general del dataset**

El resumen de las variables del dataset y su número de registros se muestran a continuación:


"""

# Ejecutar para visuzalizar la información general del Dataset
datos.info()#14 columnas

"""### **2. Descripción de los datos**

La siguiente descripción incial del dataset permite conocer las posibles variables quese pueden analizar, que conlleve a una selección de los datos de acuerdo a los objetivos del proyecto. Las caracterísitcas mas relevantes para nuesntro análisis son: 

* **Date:** La fecha de la observación
* **AveragePrice:** Precio promedio de un solo aguacate
* **Total Volume:** Número total de aguacates vendidos
* **PLU4046:** Número total de aguacates vendidos con PLU 4046 
* **PLU4225:** Número total de aguacates vendidos con PLU 4225 
* **PLU4770:** Número total de aguacates vendidos con PLU 4770 
* **type:** Convencional u orgánico
* **year:** Año
* **Region:** Ciudad o región de la observación

Los códigos de búsqueda de productos (PLU) de la tabla son solo para aguacates Hass, otra clase de aguacates no fueron consignados en esta tabla. 

Las características **Total Bags, Small Bags, Large Bags y Xlarge Bags** corresponde al número de bolsas de aguacates vendidos en la fecha correspondiente

"""

datos.head(10)

"""Se observa que la mayoria de variables son númericas y las variables categóricas son Type y Región.

### **3. Exploración y calidad de los datos**

En esta fase se realizará una exploración inicial del dataset que permita conocer algunos aspectos relevantes de los datos, permitiendo una mayor comprensión del proceso de venta del aguacáte en Estados Unidos. La siguiente tabla muestra algunas estadísticas descriptivas del conjunto de datos, teneindo en cuenta únicamente las variables númericas.
"""

datos.describe().round(4)

"""Algunas observaciones generales de los datos.

1. El  precio promedio de un aguacate tipo Hass es de 1.40 dólares aproximadamente.
2. El precio máximo  de un aguacate tipo Hass entre el 2015 y 2018 fue de 3.25 dólares y minímo de 0.44 dólares  
3. El mayor volumen de venta fue de 62505650

Las variables 'Date', 'AveragePrice', 'Total Volume', '4046', '4225', '4770', 'type' y 'región' son de gran importancia en este análisis, ya que están ligadas directamente a los objetivos del proyecto. A continuación  se muestran las tablas de frecuencia de dichas características.
"""

datos['type'].value_counts()

Grafica1= dff.pivot_table(values='AveragePrice',index='type',columns='year',aggfunc='mean',
                                   margins_name='All').round().plot(kind='bar',figsize=(12, 5),colormap='Greens_r')
Grafica1.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.xticks(rotation=0, horizontalalignment="center")
plt.title("Relación entre el año y el precio promedio respecto al tipo de aguacate")
plt.xlabel("Tipo de Aguacate")
plt.ylabel("Precio promedio por unidad")
Grafica1

datos['region'].value_counts()

plt.figure(figsize=(18,10))
sns.lineplot(x="region", y="AveragePrice",hue='year',data=dff,palette='gist_rainbow',)
plt.xticks(rotation=90, horizontalalignment="center");

datos.year.value_counts()

"""La siguiente gráfica muestra la relación entre precio promedio por unidad del aguacate clase Hass durante el pperido  2015-2018 respecto al tipo de aguacate. Observando que el valor pormedio ayor durante dicho años ocurrió en el 2017 en el aguacate tipo organico."""

sns.set_style('darkgrid')
ax = sns.catplot(x="year", y="AveragePrice", hue="type", data=datos,kind = "box",height=6, aspect=2,palette="tab10")

"""Algunas observaciones generales de los Datos.

1. Los valores de mayor precio por unidad ocurrieron en los años 2016 y 2017. 
2. El precio promedio del aguacáte orgánico ha sido superior 1.5 dólares en período entre el 2015 a 2018.
3. El aguacate tipo organico ha sido más costoso que el aguacate convencional.
4. El precio máximo del aguacáte Hass ocurrió el año 2016.

# **Fase 2. Preparación y procesamiento de los datos**

##**1. Selección de datos finales y Limpieza de datos**
Se realiza el proceso de limpieza de los datos, identificando los datos faltantes, valores incorrectos o datos con algún tipo de ruido, para dicho proceso se realizan las siguientes etapas:

**1. Identificar caracteristicas (Columnas) con datos faltantes.**
"""

Datos_faltantes =list(datos.columns[dff.isnull().any()])
print('Las características con datos faltos son:')
print(Datos_faltantes)

"""El resultado anterior muestra que el dataset no contiene datos faltantes.

**2. Filtrar las columnas para  establecer los elementos que componen dicha serie y luego realizar un conteo de cada elemento permitiendo reconocer valores con ruido ó inapropiados:**
"""

datos['AveragePrice'].loc[datos['Total Volume']>0]

datos['Total Volume'].loc[datos['Total Volume']>0]

datos['PLU4046'].value_counts()

datos['PLU4046'].value_counts().sum()

datos['PLU4225'].value_counts()

datos['PLU4225'].value_counts().sum()

datos['PLU4770'].value_counts()

datos['PLU4770'].value_counts().sum()

datos['type'].value_counts()

datos['type'].value_counts().sum()

datos['year'].value_counts()

datos['year'].value_counts().sum()

datos['region'].value_counts()

datos['region'].value_counts().sum()

"""**3. La selección de los datos finales luego de realizar la limpieza del dataset está basada en los siguientes aspectos:**

* La caracteristica 'Unnamed' se elimnan ya que es un código de referencia.

* La caracteristica 'Total Bags, Small Bags, Large Bags y Xlarge Bags no son variables de interés según los objetivos planteados.


"""

columns = ['Date','AveragePrice', 'Total Volume', 'PLU4046', 'PLU4225','PLU4770','type','year','region']
datos = datos[columns] 
datos[datos['Date'].notna()]
datos[datos['AveragePrice'].notna()]
datos[datos['Total Volume'].notna()]  
datos[datos['PLU4046'].notna()] 
datos[datos['PLU4225'].notna()]  
datos[datos['PLU4770'].notna()]   
datos[datos['type'].notna()]  
datos[datos['year'].notna()]  
datos[datos['region'].notna()]  
datos.head()

datos.info()

"""##**2. Transformación de los datos**

<p align="justify"> Con el análisis obtenido en el proceso de limpieza y filtrado, teniendo como referencia los objetivos del proyecto se observa que los datos están completos y sin ruido, únicamente se realiza un cambio en el nombre de las características para facilitar su lectura
</p> 

"""

datos.rename(columns={'Date':'Fecha'},inplace=True)
datos.rename(columns={'AveragePrice':'Precio_Promedio'},inplace=True)
datos.rename(columns={'PLU4046':'PLU_4046'},inplace=True)
datos.rename(columns={'PLU4225':'PLU_4225'},inplace=True)
datos.rename(columns={'PLU4770':'PLU_4770'},inplace=True)
datos.rename(columns={'type':'Tipo'},inplace=True)
datos.rename(columns={'year':'Año'},inplace=True)
datos.rename(columns={'region':'Región'},inplace=True)
datos.rename(columns={'Total Volume':'Volumen_Total'},inplace=True);

datos.head()

"""# **Fase 3. Análisis y modelamiento de los datos**

###**Construcción de la serie de tiempo**

De acuerdo a los datos del dataframe "avocado.cvs" procedemos a construir una serie de tiempo teniendo como referencia las caracterisiticas fecha y precio promedio.
"""

datos['Fecha'] =pd.to_datetime(datos['Fecha'])
datos.sort_values(by=['Fecha'], inplace=True, ascending=True)
serie=pd.Series(datos['Precio_Promedio'].values, index= datos['Fecha'].values);

serie

"""de acuerdo a lo anterior, procedemos a promediar los valores por semana, obteniedo un serie de los promedios por semana de los precios entre el período 2015-01-04 y 2018-03-25"""

serie_t = serie.resample('W').mean()

"""Según la serie de datos, la fecha de inicio es el 2015-01-04 y el registro se culminó el 2018-03-25, es decir un total de 169 semanas.
La gráfica muestra el comportamiento del precio promedio de aguacáte por unidad.
"""

plt.figure(figsize=(14,4), dpi = 105)
plt.plot(serie_t, label='Precio_Promedio',color='blue',linewidth=2, markersize=16,)
plt.title("Precio promedio por unidad(dólares)")
plt.legend();

"""##**Construcción de las particiones y ventanas**

Para inicar la construcción de nuestro modelo, se crea la particion de entrenamiento y prueba con una ventana de tiempo de 5 semanas.

Usaremos los registros de los primeras 118 semanas para el conjunto de entrenamiento-validación y las siguientes 51 semanas para el conjunto de pruebas.
"""

data_train = serie_t.loc[:'2017-04-02'] #Primeros 118 semanas (70% aprox)
data_test  = serie_t.loc['2017-04-03':] #ültimas 51 semanas (30% aprox).

data_train.shape ,data_test.shape #Tamaño de las particiones.

"""la siguiente función permite crear las ventanas de tiempo, según el tamaño de la ventana y los datos de la serie de tiempo."""

# Función para obtener las ventanas de tiempo.

def sliding_time(st, window_size=1):

  n = st.shape[0] - window_size  

  X = np.empty((n, window_size))
  y = np.empty(n)

  for i in range(window_size, st.shape[0]):   
    y[i - window_size] = st[i]
    X[i- window_size, 0:window_size] = np.array(st[i - window_size:i])
    
  return X, y

"""Para poder contrastar los resultados obtenidos posteriormente por el modelo, debemos crear las ventanas y valores para el entrenamiento-validación utilizando los datos entrenamiento.
De igual forma se deben crear las ventanas y valores para el entrenamiento- validación del modelo utilizando los datos de prueba, para lo cual se utiliza un tamaño de ventana de 5 semanas.
"""

k=21

X_train, y_train = sliding_time(data_train.values, window_size=k)  
print(f"Número de ejemplos de entrenamiento: {X_train.shape[0]} (Ventana de tamaño {X_train.shape[1]})")
print(f"Número de valores a predecir: {y_train.shape[0]}")

X_test, y_test = sliding_time(data_test.values, window_size=k)
print(f"Número de ejemplos de entrenamiento: {X_test.shape[0]} (Ventana de tamaño {X_test.shape[1]})")
print(f"Número de valores a predecir: {y_test.shape[0]}")

"""El valor 30 corresponde al numero de filas de la particíon de prueba menos el tamaño de la ventana

Tomando una ventana de 21 semanas, obtenemos el siguiente DataFrame
"""

#Observaciones de X en formato de DataFrame.
pd.DataFrame(X_train)



"""# **Extracción de Carécterísticas**"""

!pip install tsfresh

from tsfresh import extract_features, feature_extraction

"""## Energia

La energia se define como la cuatificación sumada de cada uno de los componenetes de la serie, claculada como: 

<img src="https://tsfresh.readthedocs.io/en/latest/_images/math/590da761ad7dc3cea32756fc232185cd32a391d6.png" width="180
" alt="Mapa de bits">
"""

print("Energia_Precio_Promedio:",feature_extraction.feature_calculators.abs_energy(datos['Precio_Promedio']))
print("Energia_Volumen_Total:",feature_extraction.feature_calculators.abs_energy(datos['Volumen_Total']))
print("Energia_PLU4046:",feature_extraction.feature_calculators.abs_energy(datos['PLU_4046']))
print("Energia_PLU_4225:",feature_extraction.feature_calculators.abs_energy(datos['PLU_4225']))
print("Energia_PLU_4770:",feature_extraction.feature_calculators.abs_energy(datos['PLU_4770']))

Precio_Promedio	Volumen_Total	PLU_4046	PLU_4225	PLU_4770	Tipo	Año	Región

"""# Kurtosis

Es una medida de normalidad que permite concoer el nivel de aproximación gaussiana de la distribución de la serie

"""

print("Curtosis_Precio_Promedio:",feature_extraction.feature_calculators.kurtosis(datos['Precio_Promedio']))
print("Curtosis_Volumen_Total:",feature_extraction.feature_calculators.kurtosis(datos['Volumen_Total']))
print("Curtosis_PLU4046:",feature_extraction.feature_calculators.kurtosis(datos['PLU_4046']))
print("Curtosis_PLU_4225:",feature_extraction.feature_calculators.kurtosis(datos['PLU_4225']))
print("Curtosis_PLU_4770:",feature_extraction.feature_calculators.kurtosis(datos['PLU_4770']))

"""# Numero de picos

cuenta el numero de piscos en una serie de tiempo (donde x-1 y x+1 son menores a x), es un descirptor completo que permite entender la frecuencia y varianza de la serie

"""

print("Número de picos_Precio_Promedio:",feature_extraction.feature_calculators.number_peaks(datos['Precio_Promedio'],3))
print("Número de picos_Volumen_Total:",feature_extraction.feature_calculators.number_peaks(datos['Volumen_Total'],3))
print("Número de picos_PLU4046:",feature_extraction.feature_calculators.number_peaks(datos['PLU_4046'],3))
print("Número de picos_PLU_4225:",feature_extraction.feature_calculators.number_peaks(datos['PLU_4225'],3))
print("Número de picos_PLU_4770:",feature_extraction.feature_calculators.number_peaks(datos['PLU_4770'],3))

"""# Coeficientes de fourier

Permite entender la relaciones frecuenciales y la composicion de la serie en cuestión, util para el analisis de fenomenos periodicos y semiperiodicos.
"""

coefs1 = feature_extraction.feature_calculators.fft_coefficient(datos['Precio_Promedio'],  [{"coeff": 1, "attr": 'real'}, {"coeff": 2, "attr": 'real'}])
coefs2 = feature_extraction.feature_calculators.fft_coefficient(datos['Volumen_Total'],  [{"coeff": 1, "attr": 'real'}, {"coeff": 2, "attr": 'real'}])
coefs3 = feature_extraction.feature_calculators.fft_coefficient(datos['PLU_4046'],  [{"coeff": 1, "attr": 'real'}, {"coeff": 2, "attr": 'real'}])
coefs4 = feature_extraction.feature_calculators.fft_coefficient(datos['PLU_4225'],  [{"coeff": 1, "attr": 'real'}, {"coeff": 2, "attr": 'real'}])
coefs5 = feature_extraction.feature_calculators.fft_coefficient(datos['PLU_4770'],  [{"coeff": 1, "attr": 'real'}, {"coeff": 2, "attr": 'real'}])
print("Coeficientes 1 y 2 de Número de Precio_Promedio:",list(coefs1))
print("Coeficientes 1 y 2 de Volumen_Total:",list(coefs2))
print("Coeficientes 1 y 2 de PLU4046:",list(coefs3))
print("Coeficientes 1 y 2 de PLU_4225:",list(coefs4))
print("Coeficientes 1 y 2 de PLU_4770:",list(coefs5))

"""## **Construcción y entrenamiento del modelo de aprendizaje supervisado**

Para este proyecto se utiliza el **Método Multilayer Perceptron Regressor (MLPR)**

<p align="justify">
Perceptron multicapa (MLP) es un algoritmo de aprendizaje supervisado que aprende una función $f (\cdot): R ^ m \rightarrow R ^ o$
 entrenando en un conjunto de datos, donde  es el número de dimensiones para la entrada y es el número de dimensiones para la salida. Dado un conjunto de características $X = {x_1, x_2, ..., x_m}$
 y un objetivo , puede aprender un aproximador de función no lineal para clasificación o regresión. </p>

<p align="justify">
MLPRegressor implementa un perceptrón multicapa (MLP) que entrena usando retropropagación sin función de activación en la capa de salida, que también se puede considerar que usa la función de identidad como función de activación. Por lo tanto, utiliza el error cuadrado como función de pérdida y la salida es un conjunto de valores continuos.</p>

**Fuente: Documentacíon scikit-learn**

Se entrena el modelo con las particiones de entrenamiento X_train, y_train
"""

from sklearn.neural_network import MLPRegressor
model = MLPRegressor(solver = 'lbfgs',
                   activation = 'relu',
                   hidden_layer_sizes=(10, 25, 100,200),
                   max_iter=3000,                   
                   n_iter_no_change=100, 
                   validation_fraction=0.2,               
                   random_state=1234)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

"""La siguiente gráfica muestra los valores predichos comparados con los valores verdaderos de la partición de pruebas."""

x = data_test.index[k:]

plt.figure(figsize=(12,4), dpi = 105)
plt.plot(x, y_test, ls = "--", label="Valor verdadero (pruebas)")
plt.plot(x, y_pred, ls = '-', label="Valor predicho (pruebas)")
plt.title("Predicción vs valores verdaderos (pruebas)")
plt.legend();

"""# **Fase 4. Evaluación y Análisis de resultados**

### **Validación Cruzada**

Utilizamos  el validador cruzado de series temporales **TimeSeriesSplit**  de scikit-learn para realizar la validación cruzada, teniedno en cuenta que se realiza validación tipo forward chaining según la cantidad de divisiones definidas en el argumento n_splits.
"""

# Definimos el número de splits para realizar cross-validation
from sklearn.model_selection import TimeSeriesSplit
tsp = TimeSeriesSplit(n_splits=k)

"""Utilizamos GridSearchCV para seleccionar los parámetros de entrenamiento del modelo, esto se realiza mediante los siguientes parámetros:

**estimator:**Nuestro modelo MLPRegressor.

**param_grid:** un diccionario en que se indicar los parámetros a evaluar como clave y el conjunto elementos como valor

**cv:** el número de conjuntos en los que se divide los datos, utilizamos TimeSeriesSplit.
"""

params = {    
      'hidden_layer_sizes' : [(180,), (80,), (130,), (100,) ], #Algunas arquitecturas propuestas.
      'activation' : ['logistic', 'tanh', 'relu'] #Funciones de activación.
 }

#Grid Search para el modelo MLPRegressor

tsp = TimeSeriesSplit(n_splits = k)

gsearch = GridSearchCV(estimator = MLPRegressor(solver = 'lbfgs', #Modelo  a explorar.          
                                                random_state=1234,
                                                max_iter= 3000,
                                                n_iter_no_change=50, 
                                                validation_fraction=0.2), 
                        cv = tsp,
                        param_grid = params, 
                        verbose = 3)

gsearch.fit(X_train, y_train)

# Función para Gráficar la predicción de los datos de precio promedio.

def plot_prediction(params, y_test, y_forward, y_last, test_date_index): 
  
  train_data = serie_t.loc[:test_date_index[0]]
  #y_test, y_forward, y_last = ys
  # Graficamos los valores predichos.
  fig = go.Figure(layout = dict(
       title = f'<b> Precio promedio semanal (2015-01-04 a 2018-03-25)</b> <br> {params}',
       dragmode= 'pan', width = 1300, height = 450))
  
  fig.add_trace(go.Scatter(x = train_data.index,  # Datos originales hasta la primer semana predicha. (fechas)
                          y = train_data.values, # Datos originales hasta la primer semana predicha. (precios)
                          mode = 'lines',
                          name = 'Valores de entrenamiento y pruebas'))

  #Gráfica de los valores de prueba reales.
  fig.add_trace(go.Scatter(x = test_date_index,
                          y = y_test,
                          mode='lines+markers',
                          name='Valores reales (y)'))


  #Gráfica de los valores predichos a partir de las ventanas de X_test.
  fig.add_trace(go.Scatter(x = test_date_index, 
                          y = y_forward,
                          mode = 'lines+markers',
                          name = 'Valores predichos a partir de datos reales'))
  
  #Gráfica de los valores predichos a partir de ventanas creadas proceduralmente.
  fig.add_trace(go.Scatter(x = test_date_index,
                          y = y_last,
                          mode='lines+markers',
                          name='Valores predichos a partir de datos predichos'))
  
  fig.show(config = dict({'scrollZoom': True}))

"""El objetivo del entrenamiento del modelo es predecir instancias futuras, para lo cual alimentaremos la serie con los datos predichos para obtener nuevos datos generados por el modelo."""

# Últimos valores de entrenamiento a usar para la predicción.
X_last = X_test[:1]

# Listas con los datos en y, empezando desde el primer valor de pruebas.
y_last =[]
y_forward = []

for i in range(len(X_test)):  
  # Valores predichos a partir de datos reales (X_test)
  y_pred_forward = gsearch.predict(X_test[i: i+1]) 
  y_forward.append(y_pred_forward[0])  
  
  # Valores predichos a partir de datos predichos y retroalimentados.
  y_pred_last = gsearch.predict(X_last)  # Se predice el valor siguiente a partir de datos predichos prevviamente.
  y_last.append(y_pred_last[0])          # Guardamos el valor predicho.

  # Creación de la nueva ventana añadiendo la última predicción.
  X_last = np.roll(X_last, -1)           # Desplazamos todos los valores hacia la izquierda con np.roll
  X_last[0,-1] = y_pred_last             # Guardamos el valor predicho en la última posición del arreglo.

"""Procedemos a graficar las dos prediciones distintas, la primera con los datos predichos y la segunda con los datos reales, utilizamos el atributo .best_params_ de GridSearchCV para tomar los mejores parámetros."""

test_date_index = data_test.index[k:]
plot_prediction(gsearch.best_params_ , y_test, y_forward, y_last, test_date_index)

"""A continuación, vamos a realizar este proceso desde la primera ventana del conjunto de evaluación, y comparar los resultados con los valores obtenidos al predecir a partir de las ventanas de evaluación y con los datos reales.

### **Métricas de Evaluación**
"""

# Datos predichos a partir de datos predichos

print(f"Test Mean Squared Error: {mean_squared_error(y_test, y_last)}")
print(f"Test Mean Absolute Error: {mean_absolute_error(y_test, y_last)}")
print(f"Test Mean squared log error: {mean_squared_log_error(y_test, y_last)}")

# Datos predichos a partir de datos reales
print(f"Test Mean Squared Error: {mean_squared_error(y_test, y_forward)}")
print(f"Test Mean Absolute Error: {mean_absolute_error(y_test, y_forward)}")
print(f"Test Mean squared log error: {mean_squared_log_error(y_test, y_forward)}")

"""#**Análisis y conclusiones preliminares**

De acuerdo al análisis previo se tienen algunas consideraciones generales:

* Lás métricas permiten establecer que el comportamiemto de los datos predichos es muy cercano a los datos de validación. 
*   El modelo tiene una aproximacion buena según las métricas de desempeño establecidas, realizando un ajuste de los hiperparámetros se puede  mejorar el ajuste.
*   El modelo puede predecir algunos comportamientos a corto plazo de de la serie de tiempo.

# **Proyecto - Avocado Markets in the United States**

* **Elaborado por:** Javier Hernan Gil Gómez - Emiliano Rodriguez

* **Docente:** Juan Sebastian Lara
"""